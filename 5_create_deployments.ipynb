{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1c8c535",
   "metadata": {},
   "source": [
    "## <span style=\"color:#ff5f27\">üë®üèª‚Äçüè´ Create Deployment </span>\n",
    "\n",
    "In this notebook, you'll create a deployment for your recommendation system.\n",
    "\n",
    "**NOTE Currently the transformer scripts are not implemented.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e13293b",
   "metadata": {},
   "source": [
    "## <span style=\"color:#ff5f27\">üìù Imports </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be13a8d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb28d839",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "571866c5",
   "metadata": {},
   "source": [
    "## <span style=\"color:#ff5f27\">üîÆ Connect to Hopsworks Feature Store </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "50414ae0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jdowling/anaconda3/envs/book/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected. Call `.close()` to terminate connection gracefully.\n",
      "\n",
      "Logged in to project, explore it here https://c.app.hopsworks.ai:443/p/17565\n",
      "Connected. Call `.close()` to terminate connection gracefully.\n"
     ]
    }
   ],
   "source": [
    "import hopsworks\n",
    "\n",
    "project = hopsworks.login()\n",
    "\n",
    "# Connect to Hopsworks Model Registry\n",
    "mr = project.get_model_registry()\n",
    "\n",
    "dataset_api = project.get_dataset_api()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50c267f1",
   "metadata": {},
   "source": [
    "## <span style=\"color:#ff5f27\">üöÄ Ranking Model Deployment </span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce4397a1",
   "metadata": {},
   "source": [
    "You start by deploying your ranking model. Since it is a CatBoost model you need to implement a `Predict` class that tells Hopsworks how to load the model and how to use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "599b7093",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(name: 'ranking_model', version: 1)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ranking_model = mr.get_best_model(\n",
    "    name=\"ranking_model\", \n",
    "    metric=\"fscore\", \n",
    "    direction=\"max\",\n",
    ")\n",
    "ranking_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c9e6918",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ranking_transformer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ranking_transformer.py\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "import hopsworks\n",
    "from opensearchpy import OpenSearch\n",
    "\n",
    "import logging\n",
    "\n",
    "\n",
    "class Transformer(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Connect to Hopsworks\n",
    "        project = hopsworks.connection().get_project()\n",
    "        self.fs = project.get_feature_store()\n",
    "        \n",
    "        # Retrieve the 'videos' feature view\n",
    "        self.videos_fv = self.fs.get_feature_view(\n",
    "            name=\"videos\", \n",
    "            version=1,\n",
    "        )\n",
    "        \n",
    "        # Get list of feature names for videos\n",
    "        self.video_features = [feat.name for feat in self.videos_fv.schema]\n",
    "        \n",
    "        # Retrieve the 'users' feature view\n",
    "        self.users_fv = self.fs.get_feature_view(\n",
    "            name=\"users\", \n",
    "            version=1,\n",
    "        )\n",
    "\n",
    "        # Retrieve the 'candidate_embeddings' feature view\n",
    "        self.candidate_index = self.fs.get_feature_view(\n",
    "            name=\"candidate_embeddings\", \n",
    "            version=1,\n",
    "        )\n",
    "\n",
    "        # Retrieve ranking model\n",
    "        mr = project.get_model_registry()\n",
    "        model = mr.get_model(\n",
    "            name=\"ranking_model\", \n",
    "            version=1,\n",
    "        )\n",
    "        \n",
    "        # Extract input schema from the model\n",
    "        input_schema = model.model_schema[\"input_schema\"][\"columnar_schema\"]\n",
    "        \n",
    "        # Get the names of features expected by the ranking model\n",
    "        self.ranking_model_feature_names = [feat[\"name\"] for feat in input_schema]\n",
    "            \n",
    "    def preprocess(self, inputs):\n",
    "        # Extract the input instance\n",
    "        inputs = inputs[\"instances\"][0]\n",
    "\n",
    "        # Extract customer_id from inputs\n",
    "        user_id = inputs[\"user_id\"]\n",
    "        \n",
    "        # Search for candidate items\n",
    "        neighbors = self.candidate_index.find_neighbors(\n",
    "            inputs[\"query_emb\"], \n",
    "            k=100,\n",
    "        )\n",
    "        neighbors = [neighbor[0] for neighbor in neighbors]\n",
    "        \n",
    "        # Get IDs of items already bought by the customer\n",
    "        already_seen_videos_ids = self.fs.sql(\n",
    "            f\"SELECT video_id from interactions_1 WHERE user_id = '{user_id}'\"\n",
    "        ).values.reshape(-1).tolist()\n",
    "        \n",
    "        # Filter candidate items to exclude those already bought by the customer\n",
    "        video_id_list = [\n",
    "            video_id\n",
    "            for video_id \n",
    "            in neighbors \n",
    "            if video_id\n",
    "            not in already_seen_videos_ids\n",
    "        ]\n",
    "        video_id_df = pd.DataFrame({\"video_id\" : video_id_list})\n",
    "        \n",
    "        # Retrieve Article data for candidate items\n",
    "        videos_data = [\n",
    "            self.videos_fv.get_feature_vector({\"video_id\": video_id}) \n",
    "            for video_id \n",
    "            in video_id_list\n",
    "        ]\n",
    "\n",
    "        videos_df = pd.DataFrame(\n",
    "            data=videos_data, \n",
    "            columns=self.video_features,\n",
    "        )\n",
    "        \n",
    "        # Join candidate items with their features\n",
    "        ranking_model_inputs = video_id_df.merge(\n",
    "            videos_df, \n",
    "            on=\"video_id\", \n",
    "            how=\"inner\",\n",
    "        )        \n",
    "        \n",
    "        # Add customer features\n",
    "        user_features = self.users_fv.get_feature_vector(\n",
    "            {\"user_id\": user_id}, \n",
    "            return_type=\"pandas\",\n",
    "        )\n",
    "        \n",
    "        ranking_model_inputs[\"user_id\"] = user_features['age'].values[0]   \n",
    "        ranking_model_inputs[\"gender\"] = user_features[\"gender\"].values[0] \n",
    "        ranking_model_inputs[\"age\"] = user_features[\"age\"].values[0] \n",
    "        ranking_model_inputs[\"country\"] = user_features[\"country\"].values[0] \n",
    "        \n",
    "        # Select only the features required by the ranking model\n",
    "        ranking_model_inputs = ranking_model_inputs[self.ranking_model_feature_names]\n",
    "                \n",
    "        return { \n",
    "            \"inputs\" : [{\"ranking_features\": ranking_model_inputs.values.tolist(), \"video_ids\": video_id_list}]\n",
    "        }\n",
    "\n",
    "    def postprocess(self, outputs):\n",
    "        # Extract predictions from the outputs\n",
    "        preds = outputs[\"predictions\"]\n",
    "        \n",
    "        # Merge prediction scores and corresponding article IDs into a list of tuples\n",
    "        ranking = list(zip(preds[\"scores\"], preds[\"video_ids\"]))\n",
    "        \n",
    "        # Sort the ranking list by score in descending order\n",
    "        ranking.sort(reverse=True)\n",
    "        \n",
    "        # Return the sorted ranking list\n",
    "        return { \n",
    "            \"ranking\": ranking,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "af0711d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading: 100.000%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4244/4244 elapsed<00:01 remaining<00:00\n"
     ]
    }
   ],
   "source": [
    "# Copy transformer file into Hopsworks File System \n",
    "uploaded_file_path = dataset_api.upload(\n",
    "    \"ranking_transformer.py\",    # File name to be uploaded\n",
    "    \"Resources\",                 # Destination directory in Hopsworks File System \n",
    "    overwrite=True,              # Overwrite the file if it already exists\n",
    ") \n",
    "\n",
    "# Construct the path to the uploaded transformer script\n",
    "transformer_script_path = os.path.join(\n",
    "    \"/Projects\",                 # Root directory for projects in Hopsworks\n",
    "    project.name,                # Name of the current project\n",
    "    uploaded_file_path,          # Path to the uploaded file within the project\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f27811f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ranking_predictor.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ranking_predictor.py\n",
    "\n",
    "import os\n",
    "import joblib\n",
    "import numpy as np\n",
    "\n",
    "import logging\n",
    "\n",
    "class Predict(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.model = joblib.load(os.environ[\"ARTIFACT_FILES_PATH\"] + \"/ranking_model.pkl\")\n",
    "\n",
    "    def predict(self, inputs):\n",
    "        # Extract ranking features and article IDs from the inputs\n",
    "        features = inputs[0].pop(\"ranking_features\")\n",
    "        video_ids = inputs[0].pop(\"video_ids\")\n",
    "        \n",
    "        # Log the extracted features\n",
    "        logging.info(\"predict -> \" + str(features))\n",
    "\n",
    "        # Predict probabilities for the positive class\n",
    "        scores = self.model.predict_proba(features).tolist()\n",
    "        \n",
    "        # Get scores of positive class\n",
    "        scores = np.asarray(scores)[:,1].tolist() \n",
    "\n",
    "        # Return the predicted scores along with the corresponding article IDs\n",
    "        return {\n",
    "            \"scores\": scores, \n",
    "            \"video_ids\": video_ids,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6f783022",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading: 100.000%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 891/891 elapsed<00:01 remaining<00:00\n"
     ]
    }
   ],
   "source": [
    "# Upload predictor file to Hopsworks\n",
    "uploaded_file_path = dataset_api.upload(\n",
    "    \"ranking_predictor.py\", \n",
    "    \"Resources\", \n",
    "    overwrite=True,\n",
    ")\n",
    "\n",
    "# Construct the path to the uploaded script\n",
    "predictor_script_path = os.path.join(\n",
    "    \"/Projects\", \n",
    "    project.name, \n",
    "    uploaded_file_path,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b600f65b",
   "metadata": {},
   "source": [
    "With that in place, you can finally deploy your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2814f952",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deployment created, explore it at https://c.app.hopsworks.ai:443/p/17565/deployments/240641\n",
      "Before making predictions, start the deployment by using `.start()`\n"
     ]
    }
   ],
   "source": [
    "from hsml.transformer import Transformer\n",
    "\n",
    "ranking_deployment_name = \"rankingdeployment\"\n",
    "\n",
    "# Define transformer\n",
    "ranking_transformer=Transformer(\n",
    "    script_file=transformer_script_path, \n",
    "    resources={\"num_instances\": 0},\n",
    ")\n",
    "\n",
    "# Deploy ranking model\n",
    "ranking_deployment = ranking_model.deploy(\n",
    "    name=ranking_deployment_name,\n",
    "    description=\"Deployment that search for video candidates and scores them based on user metadata\",\n",
    "    script_file=predictor_script_path,\n",
    "    resources={\"num_instances\": 0},\n",
    "    transformer=ranking_transformer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "687aafe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Deployment is ready: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [00:31<00:00,  5.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start making predictions by using `.predict()`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Start the deployment\n",
    "ranking_deployment.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b4bce6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check logs in case of failure\n",
    "# ranking_deployment.get_logs(component=\"predictor\", tail=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b4916bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_recommendations(ranked_candidates, k=3):\n",
    "    return [candidate[-1] for candidate in ranked_candidates['ranking'][:k]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a82db1ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a test input example\n",
    "test_ranking_input = {\"instances\": [{\n",
    "    \"user_id\": \"NT203T\",\n",
    "    \"query_emb\": [0.214135289,\n",
    "     0.571055949,\n",
    "     0.330709577,\n",
    "     -0.225899458,\n",
    "     -0.308674961,\n",
    "     -0.0115124583,\n",
    "     0.0730511621,\n",
    "     -0.495835781,\n",
    "     0.625569344,\n",
    "     -0.0438038409,\n",
    "     0.263472944,\n",
    "     -0.58485353,\n",
    "     -0.307070434,\n",
    "     0.0414443575,\n",
    "     -0.321789205,\n",
    "     0.966559],\n",
    "}]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "73181deb",
   "metadata": {},
   "outputs": [
    {
     "ename": "RestAPIError",
     "evalue": "Metadata operation error: (url: http://acfdd5a4a839249e8bb85d8b9651a20b-928877002.us-east-2.elb.amazonaws.com/v1/models/rankingdeployment:predict). Server response: \nHTTP code: 500, HTTP reason: Internal Server Error, body: b'{\"error\":\"HTTPError : HTTP 500: Feature \\'user_id\\' is missing from vector.Possible reasons: 1. There is no match in the given entry. Please check if the entry exists in the online feature store or provide the feature as passed_feature. 2. Required entries [user_id] or [user_id] are not provided.\"}', error code: , error msg: , user msg: \n\n Check the model server logs by using `.get_logs()`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRestAPIError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Test ranking deployment\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m ranked_candidates \u001b[38;5;241m=\u001b[39m \u001b[43mranking_deployment\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_ranking_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Retrieve article ids of the top recommended items\u001b[39;00m\n\u001b[1;32m      5\u001b[0m recommendations \u001b[38;5;241m=\u001b[39m get_top_recommendations(ranked_candidates, k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/book/lib/python3.10/site-packages/hsml/deployment.py:200\u001b[0m, in \u001b[0;36mDeployment.predict\u001b[0;34m(self, data, inputs)\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, data: \u001b[38;5;28mdict\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, inputs: \u001b[38;5;28mlist\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    167\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Send inference requests to the deployment.\u001b[39;00m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;124;03m       One of data or inputs parameters must be set. If both are set, inputs will be ignored.\u001b[39;00m\n\u001b[1;32m    169\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;124;03m        `dict`. Inference response.\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_serving_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/book/lib/python3.10/site-packages/hsml/engine/serving_engine.py:201\u001b[0m, in \u001b[0;36mServingEngine.predict\u001b[0;34m(self, deployment_instance, data, inputs)\u001b[0m\n\u001b[1;32m    194\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ModelServingException(\n\u001b[1;32m    195\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDeployment not created or running. If it is already created, start it by using `.start()` or check its status with .get_state()\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    196\u001b[0m     )\n\u001b[1;32m    198\u001b[0m re\u001b[38;5;241m.\u001b[39margs \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    199\u001b[0m     re\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m Check the model server logs by using `.get_logs()`\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    200\u001b[0m )\n\u001b[0;32m--> 201\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m re\n",
      "File \u001b[0;32m~/anaconda3/envs/book/lib/python3.10/site-packages/hsml/engine/serving_engine.py:185\u001b[0m, in \u001b[0;36mServingEngine.predict\u001b[0;34m(self, deployment_instance, data, inputs)\u001b[0m\n\u001b[1;32m    181\u001b[0m through_hopsworks \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    182\u001b[0m     serving_tool \u001b[38;5;241m!=\u001b[39m PREDICTOR\u001b[38;5;241m.\u001b[39mSERVING_TOOL_KSERVE\n\u001b[1;32m    183\u001b[0m )  \u001b[38;5;66;03m# if not KServe, send request to Hopsworks\u001b[39;00m\n\u001b[1;32m    184\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_serving_api\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_inference_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    186\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdeployment_instance\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpayload\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthrough_hopsworks\u001b[49m\n\u001b[1;32m    187\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m RestAPIError \u001b[38;5;28;01mas\u001b[39;00m re:\n\u001b[1;32m    189\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    190\u001b[0m         re\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m RestAPIError\u001b[38;5;241m.\u001b[39mSTATUS_CODE_NOT_FOUND\n\u001b[1;32m    191\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m re\u001b[38;5;241m.\u001b[39merror_code\n\u001b[1;32m    192\u001b[0m         \u001b[38;5;241m==\u001b[39m ModelServingException\u001b[38;5;241m.\u001b[39mERROR_CODE_DEPLOYMENT_NOT_RUNNING\n\u001b[1;32m    193\u001b[0m     ):\n",
      "File \u001b[0;32m~/anaconda3/envs/book/lib/python3.10/site-packages/hsml/core/serving_api.py:231\u001b[0m, in \u001b[0;36mServingApi.send_inference_request\u001b[0;34m(self, deployment_instance, data, through_hopsworks)\u001b[0m\n\u001b[1;32m    227\u001b[0m         _client \u001b[38;5;241m=\u001b[39m client\u001b[38;5;241m.\u001b[39mget_instance()\n\u001b[1;32m    228\u001b[0m         path_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_hopsworks_inference_path(\n\u001b[1;32m    229\u001b[0m             _client\u001b[38;5;241m.\u001b[39m_project_id, deployment_instance\n\u001b[1;32m    230\u001b[0m         )\n\u001b[0;32m--> 231\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    232\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPOST\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdumps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    233\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/book/lib/python3.10/site-packages/hsml/decorators.py:35\u001b[0m, in \u001b[0;36mconnected.<locals>.if_connected\u001b[0;34m(inst, *args, **kwargs)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m inst\u001b[38;5;241m.\u001b[39m_connected:\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m NoHopsworksConnectionError\n\u001b[0;32m---> 35\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43minst\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/book/lib/python3.10/site-packages/hsml/client/base.py:108\u001b[0m, in \u001b[0;36mClient._send_request\u001b[0;34m(self, method, path_params, query_params, headers, data, stream, files)\u001b[0m\n\u001b[1;32m    105\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_session\u001b[38;5;241m.\u001b[39msend(prepped, verify\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_verify, stream\u001b[38;5;241m=\u001b[39mstream)\n\u001b[1;32m    107\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m100\u001b[39m \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m--> 108\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mRestAPIError(url, response)\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stream:\n\u001b[1;32m    111\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "\u001b[0;31mRestAPIError\u001b[0m: Metadata operation error: (url: http://acfdd5a4a839249e8bb85d8b9651a20b-928877002.us-east-2.elb.amazonaws.com/v1/models/rankingdeployment:predict). Server response: \nHTTP code: 500, HTTP reason: Internal Server Error, body: b'{\"error\":\"HTTPError : HTTP 500: Feature \\'user_id\\' is missing from vector.Possible reasons: 1. There is no match in the given entry. Please check if the entry exists in the online feature store or provide the feature as passed_feature. 2. Required entries [user_id] or [user_id] are not provided.\"}', error code: , error msg: , user msg: \n\n Check the model server logs by using `.get_logs()`"
     ]
    }
   ],
   "source": [
    "# Test ranking deployment\n",
    "ranked_candidates = ranking_deployment.predict(test_ranking_input)\n",
    "\n",
    "# Retrieve article ids of the top recommended items\n",
    "recommendations = get_top_recommendations(ranked_candidates, k=3)\n",
    "recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ea19c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check logs in case of failure\n",
    "# ranking_deployment.get_logs(component=\"transformer\",tail=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b71f9b0",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e069e9d",
   "metadata": {},
   "source": [
    "## <span style=\"color:#ff5f27\">üöÄ Query Model Deployment </span>\n",
    "\n",
    "Next, you'll deploy your query model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "49f0cb0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the 'query_model' from the Model Registry\n",
    "query_model = mr.get_model(\n",
    "    name=\"query_model\",\n",
    "    version=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4b128224",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing querymodel_transformer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile querymodel_transformer.py\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "import hopsworks\n",
    "\n",
    "import logging\n",
    "\n",
    "\n",
    "class Transformer(object):\n",
    "    \n",
    "    def __init__(self):            \n",
    "        # Connect to the Hopsworks\n",
    "        project = hopsworks.connection().get_project()\n",
    "        ms = project.get_model_serving()\n",
    "    \n",
    "        # Retrieve the 'users' feature view\n",
    "        fs = project.get_feature_store()\n",
    "        self.users_fv = fs.get_feature_view(\n",
    "            name=\"users\", \n",
    "            version=1,\n",
    "        )\n",
    "        # Retrieve the ranking deployment \n",
    "        self.ranking_server = ms.get_deployment(\"rankingdeployment\")\n",
    "        \n",
    "        \n",
    "    def preprocess(self, inputs):\n",
    "        # Check if the input data contains a key named \"instances\"\n",
    "        # and extract the actual data if present\n",
    "        inputs = inputs[\"instances\"] if \"instances\" in inputs else inputs\n",
    "\n",
    "        # Extract customer_id from the inputs\n",
    "        user_id = inputs[\"user_id\"]\n",
    "\n",
    "        # Get customer features\n",
    "        user_features = self.users_fv.get_feature_vector(\n",
    "            {\"user_id\": user_id}, \n",
    "            return_type=\"pandas\",\n",
    "        )\n",
    "\n",
    "        # Enrich inputs with customer age\n",
    "        inputs[\"gender\"] = user_features['gender'].values[0]\n",
    "        inputs[\"age\"] = user_features['age'].values[0] \n",
    "        inputs[\"country\"] = user_features['country'].values[0]\n",
    "        \n",
    "        return {\n",
    "            \"instances\" : [inputs]\n",
    "        }\n",
    "    \n",
    "    def postprocess(self, outputs):\n",
    "        # Return ordered ranking predictions\n",
    "        return {\n",
    "            \"predictions\": self.ranking_server.predict({ \"instances\": outputs[\"predictions\"]}),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d7100357",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading: 100.000%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1620/1620 elapsed<00:01 remaining<00:00\n"
     ]
    }
   ],
   "source": [
    "# Copy transformer file into Hopsworks File System\n",
    "uploaded_file_path = dataset_api.upload(\n",
    "    \"querymodel_transformer.py\", \n",
    "    \"Models\", \n",
    "    overwrite=True,\n",
    ")\n",
    "\n",
    "# Construct the path to the uploaded script\n",
    "transformer_script_path = os.path.join(\n",
    "    \"/Projects\", \n",
    "    project.name, \n",
    "    uploaded_file_path,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a01057db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deployment created, explore it at https://c.app.hopsworks.ai:443/p/17565/deployments/240642\n",
      "Before making predictions, start the deployment by using `.start()`\n"
     ]
    }
   ],
   "source": [
    "from hsml.transformer import Transformer\n",
    "\n",
    "query_model_deployment_name = \"querydeployment\"\n",
    "\n",
    "# Define transformer\n",
    "query_model_transformer=Transformer(\n",
    "    script_file=transformer_script_path, \n",
    "    resources={\"num_instances\": 0},\n",
    ")\n",
    "\n",
    "# Deploy the query model\n",
    "query_model_deployment = query_model.deploy(\n",
    "    name=query_model_deployment_name,\n",
    "    description=\"Deployment that generates query embeddings from user and video features using the query model\",\n",
    "    resources={\"num_instances\": 0},\n",
    "    transformer=query_model_transformer,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d3387c2",
   "metadata": {},
   "source": [
    "At this point, you have registered your deployment. To start it up you need to run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "574f5dad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deployment is already running\n"
     ]
    }
   ],
   "source": [
    "# Start the deployment\n",
    "query_model_deployment.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "869d35bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check logs in case of failure\n",
    "# query_model_deployment.get_logs(component=\"transformer\", tail=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "824b2cf9",
   "metadata": {},
   "outputs": [
    {
     "ename": "RestAPIError",
     "evalue": "Metadata operation error: (url: http://acfdd5a4a839249e8bb85d8b9651a20b-928877002.us-east-2.elb.amazonaws.com/v1/models/querydeployment:predict). Server response: \nHTTP code: 500, HTTP reason: Internal Server Error, body: b'{\"error\":\"HTTPError : HTTP 500: Feature \\'user_id\\' is missing from vector.Possible reasons: 1. There is no match in the given entry. Please check if the entry exists in the online feature store or provide the feature as passed_feature. 2. Required entries [user_id] or [user_id] are not provided.\"}', error code: , error msg: , user msg: \n\n Check the model server logs by using `.get_logs()`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRestAPIError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 9\u001b[0m\n\u001b[1;32m      2\u001b[0m data \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minstances\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\n\u001b[1;32m      4\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser_id\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLW988J\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      5\u001b[0m     }\n\u001b[1;32m      6\u001b[0m }\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Test the deployment\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m ranked_candidates \u001b[38;5;241m=\u001b[39m \u001b[43mquery_model_deployment\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Retrieve article ids of the top recommended items\u001b[39;00m\n\u001b[1;32m     12\u001b[0m recommendations \u001b[38;5;241m=\u001b[39m get_top_recommendations(\n\u001b[1;32m     13\u001b[0m     ranked_candidates[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpredictions\u001b[39m\u001b[38;5;124m'\u001b[39m], \n\u001b[1;32m     14\u001b[0m     k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m,\n\u001b[1;32m     15\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/book/lib/python3.10/site-packages/hsml/deployment.py:200\u001b[0m, in \u001b[0;36mDeployment.predict\u001b[0;34m(self, data, inputs)\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, data: \u001b[38;5;28mdict\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, inputs: \u001b[38;5;28mlist\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    167\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Send inference requests to the deployment.\u001b[39;00m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;124;03m       One of data or inputs parameters must be set. If both are set, inputs will be ignored.\u001b[39;00m\n\u001b[1;32m    169\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;124;03m        `dict`. Inference response.\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_serving_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/book/lib/python3.10/site-packages/hsml/engine/serving_engine.py:201\u001b[0m, in \u001b[0;36mServingEngine.predict\u001b[0;34m(self, deployment_instance, data, inputs)\u001b[0m\n\u001b[1;32m    194\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ModelServingException(\n\u001b[1;32m    195\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDeployment not created or running. If it is already created, start it by using `.start()` or check its status with .get_state()\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    196\u001b[0m     )\n\u001b[1;32m    198\u001b[0m re\u001b[38;5;241m.\u001b[39margs \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    199\u001b[0m     re\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m Check the model server logs by using `.get_logs()`\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    200\u001b[0m )\n\u001b[0;32m--> 201\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m re\n",
      "File \u001b[0;32m~/anaconda3/envs/book/lib/python3.10/site-packages/hsml/engine/serving_engine.py:185\u001b[0m, in \u001b[0;36mServingEngine.predict\u001b[0;34m(self, deployment_instance, data, inputs)\u001b[0m\n\u001b[1;32m    181\u001b[0m through_hopsworks \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    182\u001b[0m     serving_tool \u001b[38;5;241m!=\u001b[39m PREDICTOR\u001b[38;5;241m.\u001b[39mSERVING_TOOL_KSERVE\n\u001b[1;32m    183\u001b[0m )  \u001b[38;5;66;03m# if not KServe, send request to Hopsworks\u001b[39;00m\n\u001b[1;32m    184\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_serving_api\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_inference_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    186\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdeployment_instance\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpayload\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthrough_hopsworks\u001b[49m\n\u001b[1;32m    187\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m RestAPIError \u001b[38;5;28;01mas\u001b[39;00m re:\n\u001b[1;32m    189\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    190\u001b[0m         re\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m RestAPIError\u001b[38;5;241m.\u001b[39mSTATUS_CODE_NOT_FOUND\n\u001b[1;32m    191\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m re\u001b[38;5;241m.\u001b[39merror_code\n\u001b[1;32m    192\u001b[0m         \u001b[38;5;241m==\u001b[39m ModelServingException\u001b[38;5;241m.\u001b[39mERROR_CODE_DEPLOYMENT_NOT_RUNNING\n\u001b[1;32m    193\u001b[0m     ):\n",
      "File \u001b[0;32m~/anaconda3/envs/book/lib/python3.10/site-packages/hsml/core/serving_api.py:231\u001b[0m, in \u001b[0;36mServingApi.send_inference_request\u001b[0;34m(self, deployment_instance, data, through_hopsworks)\u001b[0m\n\u001b[1;32m    227\u001b[0m         _client \u001b[38;5;241m=\u001b[39m client\u001b[38;5;241m.\u001b[39mget_instance()\n\u001b[1;32m    228\u001b[0m         path_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_hopsworks_inference_path(\n\u001b[1;32m    229\u001b[0m             _client\u001b[38;5;241m.\u001b[39m_project_id, deployment_instance\n\u001b[1;32m    230\u001b[0m         )\n\u001b[0;32m--> 231\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    232\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPOST\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdumps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    233\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/book/lib/python3.10/site-packages/hsml/decorators.py:35\u001b[0m, in \u001b[0;36mconnected.<locals>.if_connected\u001b[0;34m(inst, *args, **kwargs)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m inst\u001b[38;5;241m.\u001b[39m_connected:\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m NoHopsworksConnectionError\n\u001b[0;32m---> 35\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43minst\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/book/lib/python3.10/site-packages/hsml/client/base.py:108\u001b[0m, in \u001b[0;36mClient._send_request\u001b[0;34m(self, method, path_params, query_params, headers, data, stream, files)\u001b[0m\n\u001b[1;32m    105\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_session\u001b[38;5;241m.\u001b[39msend(prepped, verify\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_verify, stream\u001b[38;5;241m=\u001b[39mstream)\n\u001b[1;32m    107\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m100\u001b[39m \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m--> 108\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mRestAPIError(url, response)\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stream:\n\u001b[1;32m    111\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "\u001b[0;31mRestAPIError\u001b[0m: Metadata operation error: (url: http://acfdd5a4a839249e8bb85d8b9651a20b-928877002.us-east-2.elb.amazonaws.com/v1/models/querydeployment:predict). Server response: \nHTTP code: 500, HTTP reason: Internal Server Error, body: b'{\"error\":\"HTTPError : HTTP 500: Feature \\'user_id\\' is missing from vector.Possible reasons: 1. There is no match in the given entry. Please check if the entry exists in the online feature store or provide the feature as passed_feature. 2. Required entries [user_id] or [user_id] are not provided.\"}', error code: , error msg: , user msg: \n\n Check the model server logs by using `.get_logs()`"
     ]
    }
   ],
   "source": [
    "# Define a test input example\n",
    "data = {\n",
    "    \"instances\": {\n",
    "        \"user_id\": \"LW988J\",\n",
    "    }\n",
    "}\n",
    "\n",
    "# Test the deployment\n",
    "ranked_candidates = query_model_deployment.predict(data)\n",
    "\n",
    "# Retrieve article ids of the top recommended items\n",
    "recommendations = get_top_recommendations(\n",
    "    ranked_candidates['predictions'], \n",
    "    k=3,\n",
    ")\n",
    "recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ec46781c-c440-4e17-9700-cdfd4f50ec27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explore all the logs and filters in the Kibana logs at https://c.app.hopsworks.ai:443/p/17565/deployments/240642\n",
      "\n"
     ]
    },
    {
     "ename": "RestAPIError",
     "evalue": "Metadata operation error: (url: https://c.app.hopsworks.ai/hopsworks-api/api/project/17565/serving/240642/logs). Server response: \nHTTP code: 404, HTTP reason: Not Found, body: b'{\"errorCode\":240027,\"errorMsg\":\"Server logs not available\"}', error code: 240027, error msg: Server logs not available, user msg: ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRestAPIError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mquery_model_deployment\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_logs\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/book/lib/python3.10/site-packages/hsml/deployment.py:224\u001b[0m, in \u001b[0;36mDeployment.get_logs\u001b[0;34m(self, component, tail)\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m component \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m components:\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    219\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mComponent \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is not valid. Possible values are \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    220\u001b[0m             component, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(components)\n\u001b[1;32m    221\u001b[0m         )\n\u001b[1;32m    222\u001b[0m     )\n\u001b[0;32m--> 224\u001b[0m logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_serving_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_logs\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcomponent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtail\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    225\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m logs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    226\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m log \u001b[38;5;129;01min\u001b[39;00m logs:\n",
      "File \u001b[0;32m~/anaconda3/envs/book/lib/python3.10/site-packages/hsml/engine/serving_engine.py:537\u001b[0m, in \u001b[0;36mServingEngine.get_logs\u001b[0;34m(self, deployment_instance, component, tail)\u001b[0m\n\u001b[1;32m    529\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDeployment is starting, server logs might not be ready yet\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    531\u001b[0m \u001b[38;5;28mprint\u001b[39m(\n\u001b[1;32m    532\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExplore all the logs and filters in the Kibana logs at \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    533\u001b[0m     \u001b[38;5;241m+\u001b[39m deployment_instance\u001b[38;5;241m.\u001b[39mget_url(),\n\u001b[1;32m    534\u001b[0m     end\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    535\u001b[0m )\n\u001b[0;32m--> 537\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_serving_api\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_logs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdeployment_instance\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcomponent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtail\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/book/lib/python3.10/site-packages/hsml/core/serving_api.py:318\u001b[0m, in \u001b[0;36mServingApi.get_logs\u001b[0;34m(self, deployment_instance, component, tail)\u001b[0m\n\u001b[1;32m    310\u001b[0m path_params \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    311\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mproject\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    312\u001b[0m     _client\u001b[38;5;241m.\u001b[39m_project_id,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    315\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogs\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    316\u001b[0m ]\n\u001b[1;32m    317\u001b[0m query_params \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcomponent\u001b[39m\u001b[38;5;124m\"\u001b[39m: component, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtail\u001b[39m\u001b[38;5;124m\"\u001b[39m: tail}\n\u001b[0;32m--> 318\u001b[0m server_logs \u001b[38;5;241m=\u001b[39m \u001b[43m_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    319\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mGET\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery_params\u001b[49m\n\u001b[1;32m    320\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    321\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m deployable_component_logs\u001b[38;5;241m.\u001b[39mDeployableComponentLogs\u001b[38;5;241m.\u001b[39mfrom_response_json(\n\u001b[1;32m    322\u001b[0m     server_logs\n\u001b[1;32m    323\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/book/lib/python3.10/site-packages/hsml/decorators.py:35\u001b[0m, in \u001b[0;36mconnected.<locals>.if_connected\u001b[0;34m(inst, *args, **kwargs)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m inst\u001b[38;5;241m.\u001b[39m_connected:\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m NoHopsworksConnectionError\n\u001b[0;32m---> 35\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43minst\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/book/lib/python3.10/site-packages/hsml/client/base.py:108\u001b[0m, in \u001b[0;36mClient._send_request\u001b[0;34m(self, method, path_params, query_params, headers, data, stream, files)\u001b[0m\n\u001b[1;32m    105\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_session\u001b[38;5;241m.\u001b[39msend(prepped, verify\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_verify, stream\u001b[38;5;241m=\u001b[39mstream)\n\u001b[1;32m    107\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m100\u001b[39m \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m--> 108\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mRestAPIError(url, response)\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stream:\n\u001b[1;32m    111\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "\u001b[0;31mRestAPIError\u001b[0m: Metadata operation error: (url: https://c.app.hopsworks.ai/hopsworks-api/api/project/17565/serving/240642/logs). Server response: \nHTTP code: 404, HTTP reason: Not Found, body: b'{\"errorCode\":240027,\"errorMsg\":\"Server logs not available\"}', error code: 240027, error msg: Server logs not available, user msg: "
     ]
    }
   ],
   "source": [
    "query_model_deployment.get_logs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ae3e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check logs in case of failure\n",
    "# query_model_deployment.get_logs(component=\"transformer\",tail=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "245cca16",
   "metadata": {},
   "source": [
    "Stop the deployment when you're not using it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c5fe330",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop the ranking model deployment\n",
    "ranking_deployment.stop()\n",
    "\n",
    "# Stop the query model deployment\n",
    "query_model_deployment.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f71022be",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea7bc57a-abc9-491d-bddc-644e5ce0cd37",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
